# OpenXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering

[![Paper](https://img.shields.io/badge/arXiv-2507.09155-b31b1b.svg)](https://arxiv.org/abs/2507.09155)
[![Website](https://img.shields.io/badge/Website-OpenXRD-blue)](https://niaz60.github.io/OpenXRD/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Dataset](#dataset)
- [Usage](#usage)
- [Evaluation Scripts](#evaluation-scripts)
- [Results](#results)
- [Citation](#citation)
- [Contributing](#contributing)
- [License](#license)

## ğŸ”¬ Overview

OpenXRD presents an open-book pipeline designed for crystallography question answering, integrating textual prompts with concise supporting content. Instead of using scanned textbooks (which may lead to copyright issues), OpenXRD generates compact, domain-specific references that help smaller models understand key concepts in X-ray diffraction (XRD).

### Key Findings
- **Mid-capacity models** (7B-34B parameters) benefit most from external knowledge
- **Up to +11.5%** improvement with expert-reviewed materials vs +6% with AI-generated content alone
- **Inverted U relationship**: Largest models show minimal improvement while smallest models have limited capacity to utilize additional information

## âœ¨ Features

- **Comprehensive Benchmark**: 217 expert-level XRD multiple-choice questions
- **Dual Evaluation Modes**: Closed-book vs Open-book evaluation framework
- **Multiple Model Support**: GPT-4, O1, O3, LLaVA, Gemini, and more
- **Expert-Reviewed Materials**: AI-generated supporting content refined by crystallography experts
- **Detailed Analysis**: Subtask-level performance analysis and visualization tools

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- Required API keys (see Configuration section)

### Setup
```bash
git clone https://github.com/niaz60/OpenXRD.git
cd OpenXRD
pip install -r requirements.txt
```

### Configuration
Create a `.env` file in the root directory:
```bash
# OpenAI API Key (for GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# Google API Key (for Gemini models)
GEMINI_API_KEY=your_gemini_api_key_here

# Hugging Face Token (for LLaVA models, optional)
HUGGINGFACE_TOKEN=your_hf_token_here
```

## ğŸ“Š Dataset

The repository includes three main dataset files in the `datasets/` directory:

- **`benchmarking_questions.json`**: 217 expert-curated XRD questions with explanations and subtask labels
- **`supporting_textual_materials_generated.json`**: AI-generated supporting materials for open-book evaluation
- **`supporting_textual_materials_expert_reviewed.json`**: Expert-refined supporting materials

### Dataset Structure
```json
{
  "question": "What happens if the path difference between scattered rays is nÎ»?",
  "options": ["They cancel each other", "They reinforce each other", "...", "..."],
  "correct_answer": 1,
  "explanation": "When two waves differ in path by nÎ»...",
  "category": "Wave Physics",
  "subtask": "Interference Phenomena"
}
```

## ğŸ’» Usage

### Quick Start
```python
from src.evaluation import evaluate_model
from src.utils import load_dataset

# Load dataset
questions = load_dataset("datasets/benchmarking_questions.json")

# Run evaluation (example with OpenAI)
results = evaluate_model(
    model_type="openai",
    model_name="gpt-4",
    questions=questions,
    mode="closedbook"  # or "openbook"
)

print(f"Accuracy: {results['accuracy']:.2%}")
```

### Running Evaluations

#### OpenAI Models (GPT-4, O1, O3)
```bash
python scripts/evaluate_openai.py --model gpt-4 --mode closedbook
python scripts/evaluate_openai.py --model gpt-4 --mode openbook
```

#### Gemini Models
```bash
python scripts/evaluate_gemini.py --model gemini-2.0-flash --mode closedbook
```

#### LLaVA Models
```bash
python scripts/evaluate_llava.py --model llava-v1.6-34b --mode openbook
```

#### Batch Evaluation
```bash
python scripts/run_all_evaluations.py
```

## ğŸ“ Evaluation Scripts

### Core Evaluation Scripts
- `scripts/evaluate_openai.py` - OpenAI models (GPT-4, O1, O3)
- `scripts/evaluate_gemini.py` - Google Gemini models
- `scripts/evaluate_llava.py` - LLaVA vision-language models
- `scripts/evaluate_llava_next.py` - LLaVA-NeXT models

### Analysis Scripts
- `scripts/analyze_subtasks.py` - Detailed subtask-level analysis
- `scripts/generate_wordcloud.py` - Visualization of subtask distribution
- `scripts/universal_subtask_analysis.py` - Cross-model comparison

### Utility Scripts
- `scripts/generate_docx.py` - Export questions to Word document
- `scripts/reasoner_cheater.py` - Generate supporting materials

## ğŸ“ˆ Results

### Model Performance (Closed-book)
| Rank | Model | Accuracy |
|------|-------|----------|
| 1 | GPT-4.5-preview | 93.09% |
| 2 | O3-mini | 88.94% |
| 3 | O1 | 87.56% |
| 4 | GPT-4-turbo | 83.41% |
| 5 | LLaVA-v1.6-34B | 66.80% |

### Improvement with Expert-Reviewed Materials
| Model | Closed-book | Open-book | Improvement |
|-------|-------------|-----------|-------------|
| LLaVA-v1.6-34B | 66.80% | 78.30% | +11.50% |
| LLaVA-v1.6-mistral-7B | 53.00% | 64.10% | +11.10% |
| O3-mini | 88.94% | 89.90% | +0.96% |

## ğŸ”§ Project Structure

```
OpenXRD/
â”œâ”€â”€ datasets/                          # Dataset files
â”‚   â”œâ”€â”€ benchmarking_questions.json
â”‚   â”œâ”€â”€ supporting_textual_materials_generated.json
â”‚   â””â”€â”€ supporting_textual_materials_expert_reviewed.json
â”œâ”€â”€ src/                              # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluation.py                 # Main evaluation functions
â”‚   â”œâ”€â”€ models/                       # Model-specific implementations
â”‚   â”‚   â”œâ”€â”€ openai_models.py
â”‚   â”‚   â”œâ”€â”€ gemini_models.py
â”‚   â”‚   â””â”€â”€ llava_models.py
â”‚   â””â”€â”€ utils.py                      # Utility functions
â”œâ”€â”€ scripts/                          # Evaluation scripts
â”‚   â”œâ”€â”€ evaluate_openai.py
â”‚   â”œâ”€â”€ evaluate_gemini.py
â”‚   â”œâ”€â”€ evaluate_llava.py
â”‚   â”œâ”€â”€ analyze_subtasks.py
â”‚   â””â”€â”€ run_all_evaluations.py
â”œâ”€â”€ results/                          # Output directory
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env.example                      # Environment variables template
â””â”€â”€ README.md                         # This file
```

## ğŸ“š Citation

If you use OpenXRD in your research, please cite our paper:

```bibtex
@article{vosoughi2025openxrd,
  title={OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering},
  author={Vosoughi, Ali and Shahnazari, Ayoub and Xi, Yufeng and Zhang, Zeliang and Hess, Griffin and Xu, Chenliang and Abdolrahim, Niaz},
  journal={arXiv preprint arXiv:2507.09155},
  year={2025}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors

- **Ali Vosoughi** - University of Rochester
- **Ayoub Shahnazari** - University of Rochester  
- **Yufeng Xi** - University of Rochester
- **Zeliang Zhang** - University of Rochester
- **Griffin Hess** - University of Rochester
- **Chenliang Xu** - University of Rochester
- **Niaz Abdolrahim** - University of Rochester

## ğŸ™ Acknowledgments

- National Nuclear Security Administration (NNSA) under grant NA0004078
- National Science Foundation (NSF) under grant 2202124
- Department of Energy (DOE) under award DE-SC0020340

## ğŸ“ Contact

For questions or collaboration opportunities, please open an issue or contact the maintainers.

---

**Project Website**: [https://niaz60.github.io/OpenXRD/](https://niaz60.github.io/OpenXRD/)
